{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#import sys\n",
    "#if len(sys.argv) != 4:\n",
    "#    print('Usage:')\n",
    "#    print('python train.py datacfg cfgfile weightfile')\n",
    "#    exit()\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import dataset\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from utils import *\n",
    "from cfg import parse_cfg\n",
    "from region_loss import RegionLoss\n",
    "from darknet import Darknet\n",
    "from models.tiny_yolo import TinyYoloNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "datacfg       = 'cfg/voc.data'#sys.argv[1]\n",
    "#cfgfile       = 'cfg/tiny-yolo-voc.cfg'#sys.argv[2]\n",
    "cfgfile       = 'cfg/tiny-yolo-hackathon.cfg'\n",
    "#weightfile    = 'darknet19_448.conv.23'#sys.argv[3]\n",
    "weightfile    = 'cfg/yolov2-tiny-voc.weights'\n",
    "\n",
    "data_options  = read_data_cfg(datacfg)\n",
    "net_options   = parse_cfg(cfgfile)[0]\n",
    "\n",
    "trainlist     = data_options['train']\n",
    "testlist      = data_options['valid']\n",
    "backupdir     = data_options['backup']\n",
    "nsamples      = file_lines(trainlist)\n",
    "gpus          = data_options['gpus']  # e.g. 0,1,2,3\n",
    "ngpus         = len(gpus.split(','))\n",
    "num_workers   = int(data_options['num_workers'])\n",
    "\n",
    "batch_size    = int(net_options['batch'])\n",
    "max_batches   = int(net_options['max_batches'])\n",
    "learning_rate = float(net_options['learning_rate'])\n",
    "momentum      = float(net_options['momentum'])\n",
    "decay         = float(net_options['decay'])\n",
    "steps         = [float(step) for step in net_options['steps'].split(',')]\n",
    "scales        = [float(scale) for scale in net_options['scales'].split(',')]\n",
    "\n",
    "#Train parameters\n",
    "max_epochs    = max_batches*batch_size/nsamples+1\n",
    "use_cuda      = True\n",
    "seed          = int(time.time())\n",
    "eps           = 1e-5\n",
    "save_interval = 10  # epoches\n",
    "dot_interval  = 70  # batches\n",
    "\n",
    "# Test parameters\n",
    "conf_thresh   = 0.25\n",
    "nms_thresh    = 0.4\n",
    "iou_thresh    = 0.5\n",
    "\n",
    "if not os.path.exists(backupdir):\n",
    "    os.mkdir(backupdir)\n",
    "    \n",
    "###############\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "model       = Darknet(cfgfile)\n",
    "region_loss = model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/home/hack/yolo2_onnx/cfg.py:179: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  conv_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_w])); start = start + num_w #20180615 modified for <= torch v0.3.0 post4\n",
      "/notebooks/home/hack/yolo2_onnx/cfg.py:158: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  conv_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_w])); start = start + num_w #20180615 modified for <= torch v0.3.0 post4\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(weightfile)\n",
    "#model.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2566400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.seen = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_loss.seen  = model.seen\n",
    "processed_batches = model.seen/batch_size\n",
    "\n",
    "init_width        = model.width\n",
    "init_height       = model.height\n",
    "init_epoch        = model.seen/nsamples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = list(model.children())[0]\n",
    "for item in list(layers)[:-2]:\n",
    "    item.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_model = model\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset.listDataset(trainlist, shape=(init_width, init_height),\n",
    "                   shuffle=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ]), \n",
    "                   train=True, \n",
    "                   seen=cur_model.seen,\n",
    "                   batch_size=batch_size,\n",
    "                   num_workers=num_workers),\n",
    "    batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data, target) = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = Variable(data), Variable(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_loss(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': num_workers, 'pin_memory': True} if use_cuda else {}\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset.listDataset(testlist, shape=(init_width, init_height),\n",
    "                   shuffle=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ]), train=False),\n",
    "    batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "if use_cuda:\n",
    "    if ngpus > 1:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "    else:\n",
    "        model = model.cuda()\n",
    "\n",
    "params_dict = dict(model.named_parameters())\n",
    "params = []\n",
    "for key, value in params_dict.items():\n",
    "    if key.find('.bn') >= 0 or key.find('.bias') >= 0:\n",
    "        params += [{'params': [value], 'weight_decay': 0.0}]\n",
    "    else:\n",
    "        params += [{'params': [value], 'weight_decay': decay*batch_size}]\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate/batch_size, momentum=momentum, dampening=0, weight_decay=decay*batch_size)\n",
    "\n",
    "def adjust_learning_rate(optimizer, batch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = learning_rate\n",
    "    for i in range(len(steps)):\n",
    "        scale = scales[i] if i < len(scales) else 1\n",
    "        if batch >= steps[i]:\n",
    "            lr = lr * scale\n",
    "            if batch == steps[i]:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr/batch_size\n",
    "    return lr\n",
    "\n",
    "def train(epoch):\n",
    "    #pdb.set_trace()\n",
    "    global processed_batches\n",
    "    t0 = time.time()\n",
    "    if ngpus > 1:\n",
    "        cur_model = model.module\n",
    "    else:\n",
    "        cur_model = model\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset.listDataset(trainlist, shape=(init_width, init_height),\n",
    "                       shuffle=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ]), \n",
    "                       train=True, \n",
    "                       seen=cur_model.seen,\n",
    "                       batch_size=batch_size,\n",
    "                       num_workers=num_workers),\n",
    "        batch_size=batch_size, shuffle=False, drop_last=True, **kwargs)\n",
    "\n",
    "    lr = adjust_learning_rate(optimizer, processed_batches)\n",
    "    logging('epoch %d, processed %d samples, lr %f' % (epoch, epoch * len(train_loader.dataset), lr))\n",
    "    model.train()\n",
    "    t1 = time.time()\n",
    "    avg_time = torch.zeros(9)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        t2 = time.time()\n",
    "        adjust_learning_rate(optimizer, processed_batches)\n",
    "        processed_batches = processed_batches + 1\n",
    "        #if (batch_idx+1) % dot_interval == 0:\n",
    "        #    sys.stdout.write('.')\n",
    "\n",
    "        if use_cuda:\n",
    "            data = data.cuda()\n",
    "            #target= target.cuda()\n",
    "        t3 = time.time()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        t4 = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        t5 = time.time()\n",
    "        output = model(data)\n",
    "        t6 = time.time()\n",
    "        region_loss.seen = region_loss.seen + data.data.size(0)\n",
    "        loss = region_loss(output, target)\n",
    "        t7 = time.time()\n",
    "        loss.backward()\n",
    "        t8 = time.time()\n",
    "        optimizer.step()\n",
    "        t9 = time.time()\n",
    "        if False and batch_idx > 1:\n",
    "            avg_time[0] = avg_time[0] + (t2-t1)\n",
    "            avg_time[1] = avg_time[1] + (t3-t2)\n",
    "            avg_time[2] = avg_time[2] + (t4-t3)\n",
    "            avg_time[3] = avg_time[3] + (t5-t4)\n",
    "            avg_time[4] = avg_time[4] + (t6-t5)\n",
    "            avg_time[5] = avg_time[5] + (t7-t6)\n",
    "            avg_time[6] = avg_time[6] + (t8-t7)\n",
    "            avg_time[7] = avg_time[7] + (t9-t8)\n",
    "            avg_time[8] = avg_time[8] + (t9-t1)\n",
    "            print('-------------------------------')\n",
    "            print('       load data : %f' % (avg_time[0]/(batch_idx)))\n",
    "            print('     cpu to cuda : %f' % (avg_time[1]/(batch_idx)))\n",
    "            print('cuda to variable : %f' % (avg_time[2]/(batch_idx)))\n",
    "            print('       zero_grad : %f' % (avg_time[3]/(batch_idx)))\n",
    "            print(' forward feature : %f' % (avg_time[4]/(batch_idx)))\n",
    "            print('    forward loss : %f' % (avg_time[5]/(batch_idx)))\n",
    "            print('        backward : %f' % (avg_time[6]/(batch_idx)))\n",
    "            print('            step : %f' % (avg_time[7]/(batch_idx)))\n",
    "            print('           total : %f' % (avg_time[8]/(batch_idx)))\n",
    "        t1 = time.time()\n",
    "    print('')\n",
    "    t1 = time.time()\n",
    "    logging('training with %f samples/s' % (len(train_loader.dataset)/(t1-t0)))\n",
    "    if (epoch+1) % save_interval == 0:\n",
    "        logging('save weights to %s/%06d.weights' % (backupdir, epoch+1))\n",
    "        cur_model.seen = (epoch + 1) * len(train_loader.dataset)\n",
    "        cur_model.save_weights('%s/%06d.weights' % (backupdir, epoch+1))\n",
    "\n",
    "def test(epoch):\n",
    "    def truths_length(truths):\n",
    "        for i in range(50):\n",
    "            if truths[i][1] == 0:\n",
    "                return i\n",
    "\n",
    "    model.eval()\n",
    "    if ngpus > 1:\n",
    "        cur_model = model.module\n",
    "    else:\n",
    "        cur_model = model\n",
    "    num_classes = cur_model.num_classes\n",
    "    anchors     = cur_model.anchors\n",
    "    num_anchors = cur_model.num_anchors\n",
    "    total       = 0.0\n",
    "    proposals   = 0.0\n",
    "    correct     = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        output = model(data).data\n",
    "        all_boxes = get_region_boxes(output, conf_thresh, num_classes, anchors, num_anchors)\n",
    "        for i in range(output.size(0)):\n",
    "            boxes = all_boxes[i]\n",
    "            boxes = nms(boxes, nms_thresh)\n",
    "            truths = target[i].view(-1, 5)\n",
    "            num_gts = truths_length(truths)\n",
    "     \n",
    "            total = total + num_gts\n",
    "    \n",
    "            for i in range(len(boxes)):\n",
    "                if boxes[i][4] > conf_thresh:\n",
    "                    proposals = proposals+1\n",
    "\n",
    "            for i in range(num_gts):\n",
    "                box_gt = [truths[i][1], truths[i][2], truths[i][3], truths[i][4], 1.0, 1.0, truths[i][0]]\n",
    "                best_iou = 0\n",
    "                best_j = -1\n",
    "                for j in range(len(boxes)):\n",
    "                    iou = bbox_iou(box_gt, boxes[j], x1y1x2y2=False)\n",
    "                    if iou > best_iou:\n",
    "                        best_j = j\n",
    "                        best_iou = iou\n",
    "                if best_iou > iou_thresh and boxes[best_j][6] == box_gt[6]:\n",
    "                    correct = correct+1\n",
    "\n",
    "    precision = 1.0*correct/(proposals+eps)\n",
    "    recall = 1.0*correct/(total+eps)\n",
    "    fscore = 2.0*precision*recall/(precision+recall+eps)\n",
    "    logging(\"precision: %f, recall: %f, fscore: %f\" % (precision, recall, fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4952"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate = False\n",
    "if evaluate:\n",
    "    logging('evaluating ...')\n",
    "    test(0)\n",
    "else:\n",
    "#    for epoch in range(int(init_epoch), int(max_epochs)): \n",
    "    for epoch in range(int(init_epoch), int(1)): \n",
    "        train(epoch)\n",
    "        test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
